# Big Op and little op

## Asymptotic notation

"Big Op" (*big oh-pee*), or in algebraic terms $O_p$, is a shorthand means of characterising the convergence in probability of a set of random variables. It directly builds on the same sort of convergence ideas that were discussed in Chapters [5](#wlln) and [6](#slutsky).

Big Op means that some given random variable is stochastically bounded. If we have some set of random variable $X_n$ and some set of constants $a_n$ (where n indexes both sets), then 

$$ X_n = O_p(a_n)$$ is the same as saying that $$ P(|\frac{X_n}{a_n}| > M) < \epsilon, \forall n > N. $$
$M \text{ and } N$ here are just finite numbers, and $\epsilon$ is some arbitrary (small) number. In plain English, Big Op means that for a large enough $n$ there is some number ($M$) such that the probability that the random variable $\frac{X_n}{a_n}$ is larger than that number is essentially zero. In other words, $\frac{X_n}{a_n}$ converges on some upper bound given a sufficiently large $n$.

Little op, by contrast, refers to convergence in probability towards zero. If $X_n = o_p(a_n)$ then

$$ \frac{x_n}{a_n} = o_p(1) $$ 

where $X_n = o_p(1)$ is the same as

$$ \lim_{n\to\infty} (P|X_n| \geq \epsilon) = 0, \forall\epsilon > 0. $$

Hence, to provide one final rearrangement, we can say that $X_n = o_p(a_n)$ is the same as

$$ \lim_{n\to\infty} (P|\frac{X_n}{a_n}| \geq \epsilon) = 0, \forall\epsilon > 0.$$
In other words, $X_n = o_p(a_n)$ if and only if $\frac{X_n}{a_n} \xrightarrow{p} 0$.

### Relationship of big-O and little-o

The definitions above are perhaps the easiest way to consider these two different forms of converging random variables. But, in fact, we can re-express $o_p$ (again!) to make clear how it diverges from $O_p$.

From above, we can generalise the definition of $X_n = O_p(a_n)$ as

$$\forall \epsilon\;\; \exists N_\epsilon,\delta_\epsilon \;\; s.t. \forall n > N_\epsilon,\;\; P(|\frac{X_n}{a_n}| \geq \delta_\epsilon) \leq \epsilon.$$
This restatement makes it clear that the values of $\delta$ and $N$ can be found with respect to any given $\epsilon$. That is, we only have to find one value of $N$ and $\delta$ and these can differ as we select a new $\epsilon$. 

Similarly, we can re-express $X_n = o_p(a_n)$ as 

$$\forall \epsilon,\delta\;\; \exists N_{\epsilon,\delta} \;\; s.t. \forall n > N_{\epsilon,\delta},\;\; P(|\frac{X_n}{a_n}| \geq \delta) \leq \epsilon.$$
Hence, $O_p$ and $o_p$ are very similar, except for how they range over $\delta$ and $N$. In the case of $o_p$, we make a more general statement by saying that for any value of $\epsilon$ and $\delta$, and any combination of those two values, there is some $N$ that satisfies the above inequality (assuming $X_n = o_p(a_n)$). 

Note also, therefore that $o_p(a_n)$ entails $O_p(a_n)$, but that the inverse is not true. If for all $\epsilon$ and $\delta$ there is some $N_{\epsilon,\delta}$ that satisfies the inequality, then it must be the case that for all $\epsilon$ there exists some $\delta$ such that the inequality also holds. But just because for some $\delta_\epsilon$ the inequality holds, this does not mean that it will hold for *all* $\delta$.

## Why is this useful?[^fn_cite1]

[^fn_cite1]:
   The first two examples in this section are adapted from Ashesh Rambachan's [Asymptotics Review lecture slides](https://scholar.harvard.edu/files/asheshr/files/asymptotics-slides.pdf), from Harvard Math Camp -- Econometrics 2018.

A simple example of this notation is to consider the random variable $X_n$ that has known $\mathbb{E}[X_n] = X$. We can therefore decompose $X_n = X + o_p(1)$, since we know by the [Weak Law of Large Numbers](#wlln) that $X_n \xrightarrow{p} X$. This is useful because, without having to introduce explicit limits into our equations, we know that with a sufficiently large $n$, the second term of our decomposition converges to zero, and therefore we can (in a hand-wavey fashion) ignore it.

Let's consider a more meaningful example. Suppose now that $X_n \sim N(0,v)$. Using known features of normal distributions, we can rearrange this to

$$ \frac{X_n}{\sqrt{v}} \sim N(0,1). $$
This distribution tends in probability to zero as $n \to \infty$ by definition, and therefore 

$$ X_n = O_p(\sqrt{v}).$$

$X_n$ is also $o_p(v)$ because

$$ \frac{X_n}{v} \sim N(0,\frac{v}{v^2}) = N(0,\frac{1}{v})$$
$$ \lim_{n\to\infty}P\left( \left| \frac{N(0,\frac{1}{v})}{v} \right| \geq \epsilon \right) = 0 = o_p(1), $$

for all $\epsilon > 0$, and therefore that

$$X_n = o_p(v)$$.

The big-O, little-o notation captures the complexity of the equation or, equivalently, the rate at which it converges. One way to read $X_n = o_p(a_n)$ is that, for any multiple of $j$, $X_n$ converges in probability to zero at the rate determined by $a_n$. So, for example, $o_p(a_n^2)$ converges faster than $o_p(a_n)$, since for some random variable $X_n$, $\frac{X_n}{a_n^2} < \frac{X_n}{a_n}, n > 1.$

This notation is useful because, when we want to work out the asymptotic limits of a more complicated equation, where multiple terms are affected by the number of observations, if we have a term that converges in probability to zero at a faster rate than others then we can safely ignore that term. Very informally, by summarising a term as $O_p(j)$ we are essentially saying that this term's effect on the outcome is bounded to be only as large as $j$, and typically we summarise those components where all other terms are of an order larger than $j$ and thus substantially important.

## Worked Example: Consistency of mean estimators {#estimator_consistency}

A parameter is "consistent" if it converges in probability to the true parameter as the number of observations increases. More formally, a parameter  $\hat{\theta}$ is consistent if 

$$ P(|\hat{\theta} - \theta| \geq \epsilon) \xrightarrow{p} 0 $$

One question we can ask is how fast our consistent parameter $\hat{\theta}$ converges on the true parameter value. This is an "applied" methods problem to the extent that, as researchers seeking to make an inference about the true parameter, and confronted with potentially many ways of estimating it, we want to choose an efficient estimator i.e. one that gets to the truth quickest!

Let's suppose we want to estimate the population mean of $X$, i.e. $\bar{X}$. Suppose further we have two potential estimators, the sample mean is $\frac{1}{N}\sum_{i=1}^N X_i$ and the median is $X_{(N+1)/2}$, where $N = 2n + 1$ (we'll assume an odd number of observations for the ease of calculation) and $Y$ is an ordered sequence from smallest to largest.

We know by the [Central Limit Theorem](#clt) that the sample mean 

$$ \bar{X}_N \sim \mathcal{N}(\theta, \frac{\sigma^2}{N}), $$

and note that I use $\mathcal{N}$ to denote the normal distribution function, to avoid confusion with $N$ the total number of observations.

Withholding the proof, the large-sample distribution of the median estimator can be expressed *approximately*[^fn_median] as

$$ Med\{X_1,X_2,...,X_N\} \sim \mathcal{N}(\theta, \frac{\pi\sigma^2}{2N}). $$

[^fn_median]:
   See [this Wolfram MathWorld post](https://mathworld.wolfram.com/StatisticalMedian.html) for more information about the exact CLT distribution of sample medians. 
   
How do these estimators perform in practice? We can check by running a simulation study in R. First, let us simulate draws of a standard normal distribution with various sizes of N, and plot the resulting distribution of the two estimators:

```{r packages, message = FALSE}
library(tidyverse)
library(ccaPP) # This pkg includes a faster implementation of calculating the median than base-R's algorithm
```


```{r, include=TRUE}
# Fun. computes sample mean and median 1000 times, using N draws from std. normal
rep_sample <- function(N) {
  sample_means <- c()
  sample_medians <- c()
  for (s in 1:1000) {
    sample <- rnorm(N)
    sample_means[s] <- mean(sample)
    sample_medians[s] <- fastMedian(sample)
  }
  return(data.frame(N = N, Mean = sample_means, Median = sample_medians))
}

set.seed(89)
Ns <- c(5,seq(50,250, by = 50)) # A series of sample sizes

# Apply function and collect results, then pivot dataset to make plotting easier
sim_results <- do.call("rbind", lapply(Ns, FUN = function(x) rep_sample(x))) %>% 
  pivot_longer(-N, names_to = "Estimator", values_to = "estimate")

ggplot(sim_results, aes(x = estimate, color = Estimator, fill = Estimator)) +
  facet_wrap(~N, ncol = 2, scales = "free_y", labeller = "label_both") +
  geom_density(alpha = 0.5) +
  labs(x = "Value", y = "Density") +
  theme(legend.position = "bottom")
  


```

Here we can see that for both the mean and median sample estimators, the distribution of parameters is normally distributed around the true mean ($\theta = 0$). The variance of the sample mean distribution, however, shrinks faster than that of the sample median estimator. In other words, the sample mean is more "efficient" (in fact it is the most efficient estimator). Efficiency here captures what we noted mathematically above -- that the rate of convergence on the true parameter (i.e. the rate at which the estimator converges on zero) is faster for the sample mean than the median.

Note that both estimators are therefore unbiased (they are centred on $\theta$), normally distributed, and are consistent (the sampling distributions shrink towards the true parameter as N increases), but that the variances shrinks at slightly different rates. 

We can quantify this using little-o notation and the behaviour of these estimators' with large-samples. First, we can define the estimation errors of the mean and median respectively as 

$$
\begin{aligned}
\psi_\text{Mean} &= |\hat{\theta} - \theta| \\ 
&= \left|\mathcal{N}(\theta, \frac{\sigma^2}{N}) - \mathcal{N}(\theta,0) \right| \\
&= \mathcal{N}(0,\frac{\sigma^2}{N}).
\end{aligned}
$$

Similarly,

$$
\begin{aligned}
\psi_\text{Med.} &= \left|\mathcal{N}(\theta, \frac{\pi\sigma^2}{2N}) - \mathcal{N}(\theta,0) \right| \\
& = \mathcal{N}(0, \frac{\pi\sigma^2}{2N}).
\end{aligned}
$$


The estimation error of both estimators therefore are centered on zero. Given earlier discussions in this chapter, we can rearrange both to find out their rate of convergence. 

For the sample mean:

$$ 
\begin{aligned}
\psi_\text{Mean} &= \frac{1}{\sqrt{N}}\mathcal{N}(0,\sigma^2) \\
\frac{\psi_\text{Mean}}{N^{-0.5}} &= \mathcal{N}(0,\sigma^2)
\end{aligned}
$$

And from before, we know that a normal distribution centred around zero is $o_p(1)$, and therefore that 

$$ \psi_\text{Mean} = o_p\left(\frac{1}{\sqrt{N}}\right).$$

Similarly, for the sample median:

$$ 
\begin{aligned}
\psi_\text{Med.} &= \mathcal{N}(0, \frac{\pi\sigma^2}{2N}) \\
&= \left( \frac{\pi}{2N}\right)^{0.5}\mathcal{N}(0,\sigma^2) \\
\psi_\text{Med.}/\left( \frac{\pi}{2N}\right)^{0.5} &= \mathcal{N}(0,\sigma^2)\\
\psi_\text{Med.} &= o_p\left( \left[\frac{\pi}{2N}\right]^{0.5}\right) \\
&= o_p\left(\frac{\sqrt{\pi}}{\sqrt{2N}}\right)
\end{aligned}
$$

And note that the little-o of the sample median's estimating error of the mean is "slower" (read: larger) than the little-o of the sample mean, meaning that the sample mean will converge on the true mean with fewer observations than the sample median. 

Finally, using the derivations of the little-o of each estimator, we can plot these orders visually:

```{r, include = TRUE}

N <- seq(0.01,5, by = 0.01)
mean_convergence <- 1/sqrt(N)
median_convergence <- sqrt(pi)/sqrt(2*N)

plot_df <- data.frame(N, Mean = mean_convergence, Median = median_convergence) %>% 
  pivot_longer(-N, names_to = "Estimator", values_to = "Rate")

ggplot(plot_df, aes(x = N, y = Rate, color = Estimator)) +
  geom_line() +
  theme(legend.position = "bottom")

```

Note that the median rate line is always above the mean line for all $N$ (though not by much) -- it therefore has a slower convergence.

## Arithmetic Operations

As a final note, I briefly detail some very basic arithmetic implications that apply to $O_p$ and $o_p$:

* $o_p(1) + o_p(1) = o_p(1)$ -- this is straightforward: two terms that both converge to zero at the same rate, collectively converge to zero at that rate. Note this is actually just an application of [Continuous Mapping Theorem](#cmt), since If $X_n = o_p(1), Y_n = o_p(1)$ then $X_n \xrightarrow{p} 0, Y_n \xrightarrow{p} 0$ then the addition of these two terms is a continuous mapping function, and therefore $X_n + Y_n \xrightarrow{p} 0, \; \therefore X_n+Y_n = o_p(1)$.

* $O_p(1) + o_p(1) = O_p(1)$ -- a term that is bounded in probability ($O_p(1)$) plus a term converging in probability to zero, is bounded in probability.

* $O_p(1)o_p(1) = o_p(1)$ -- a bounded probability multiplied by a term that converges (in the same order) to zero itself converges to zero.

* $o_p(R) = Ro_p(1)$ -- again this is easy to see, since suppose $X_n = o_p(R)$, then $X_n/R = o_p(1)$, and so $X_n = Ro_p(1)$.

These rules are intuitive, and other related rules can be found in Section 2.2 of @vaart_1998.

